{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import yaml\n",
    "import json\n",
    "import requests \n",
    "from tqdm.auto import notebook_tqdm\n",
    "import minsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('key.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = config['mistral']\n",
    "model = \"mistral-small-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[{\"role\":\"user\", \"content\":\"What is the best French cheese?\"}]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='9119f351194c46b0b9cc1b1e29d1f4cb', object='chat.completion', model='mistral-small-latest', usage=UsageInfo(prompt_tokens=10, completion_tokens=412, total_tokens=422), created=1739369757, choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='Determining the \"best\" French cheese can be quite subjective, as it often depends on personal preferences, such as whether you prefer soft, hard, blue, or goat cheeses. However, some of the most renowned and beloved French cheeses include:\\n\\n1. **Camembert de Normandie**: A soft, bloomy rind cheese with a creamy interior, often considered one of the finest examples of French cheese.\\n\\n2. **Roquefort**: A blue cheese made from sheep\\'s milk, known for its strong, tangy flavor and crumbly texture.\\n\\n3. **Brie de Meaux**: A soft cheese with a creamy texture and a rich, buttery flavor, often considered one of the finest cheeses in the world.\\n\\n4. **Comté**: A hard cheese made from cow\\'s milk, known for its nutty and fruity flavors. It is often aged for several months to develop its complex taste.\\n\\n5. **Chèvre (Goat Cheese)**: There are many varieties, including fresh, soft, and aged. They are known for their tangy and slightly tart flavor.\\n\\n6. **Beaufort**: Another hard cheese, made from cow\\'s milk, known for its rich, nutty flavor and its ability to melt beautifully.\\n\\n7. **Pont-l\\'Évêque**: A soft, creamy cheese with a washed rind, known for its pungent aroma and mild, creamy flavor.\\n\\n8. **Munster**: A soft, washed-rind cheese with a strong aroma and a slightly pungent flavor, often compared to Limburger.\\n\\n9. **Emmental**: A hard cheese with a distinctive nutty flavor and large holes, it\\'s often used in fondue.\\n\\n10. **Reblochon**: A soft, washed-rind cheese from the Alps, known for its creamy texture and mild, tangy flavor.\\n\\nEach of these cheeses has its own unique characteristics and flavor profiles, so the \"best\" one really depends on what you enjoy most.', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(chat_response.choices[0].message.content)\n",
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT * WHERE course = 'data-engineering-zoomcamp'; thats for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'the course has already started, can I still enroll?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7231f3119bb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whether you can still enroll in a course that has already started depends on the policies of the educational institution or platform offering the course. Here are some general guidelines:\\n\\n1. **Check the Course Policy**: Many institutions and platforms have specific policies regarding late enrollment. Some may allow you to join late with certain conditions, while others may have strict deadlines.\\n\\n2. **Contact the Instructor or Administration**: Reach out to the course instructor, program administrator, or support team to inquire about the possibility of late enrollment. They can provide the most accurate and up-to-date information.\\n\\n3. **Review Syllabus and Requirements**: If late enrollment is allowed, you might need to catch up on missed content. Review the course syllabus to understand the workload and any prerequisites.\\n\\n4. **Technical and Logistical Considerations**: Ensure that you have access to all necessary materials, platforms, and resources. You might also need to set up any required accounts or software.\\n\\n5. **Consider the Impact on Learning**: Joining a course late can be challenging, as you may miss out on foundational material and interactions with peers. Be prepared to put in extra effort to catch up.\\n\\n6. **Special Circumstances**: If you have a legitimate reason for being late (e.g., illness, family emergency), you might have a better chance of being allowed to enroll late.\\n\\nIn summary, while it's possible to enroll late in some cases, it's best to act quickly and contact the relevant authorities for specific instructions and approval.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": q}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "        You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "        Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "        QUESTION: {question}\n",
    "\n",
    "        CONTEXT: \n",
    "        {context}\n",
    "        \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I run kafka?'\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the CONTEXT provided, here's how you can run Kafka:\\n\\n1. **Java Kafka**: In the project directory, run:\\n   ```\\n   java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n   ```\\n\\n2. **Python Kafka**: Ensure Docker images are up and running and create a virtual environment:\\n   ```\\n   python -m venv env\\n   source env/bin/activate (or env/Scripts/activate on Windows)\\n   pip install -r ../requirements.txt\\n   ```\\n   Then run your Python files within this virtual environment:\\n   ```\\n   python <your_script>.py\\n   ```\\n   If you encounter a permission error with `./build.sh`, run:\\n   ```\\n   chmod +x build.sh\\n   ```\\n   If you encounter a `ModuleNotFoundError: No module named 'kafka.vendor.six.moves'`, install `kafka-python-ng`:\\n   ```\\n   pip install kafka-python-ng\\n   ```\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a raw database in whatever format, in this case theye google docs files. We parse those files inot a better format, more structures, in this case they are a jsoin file. we use a self made search engine to build a context for the llm, in this case we select a series of the questions that are related to the query from the user and we parse that into a prompt to the llm so it can elaborate a human like response to the query. easier to manipulate and understand. promp, query, number of responses retrieved by the search engine those kind of things are what makes the llm work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## elastic search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado a Elasticsearch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47571/395609063.py:1: DeprecationWarning: The 'timeout' parameter is deprecated in favor of 'request_timeout'\n",
      "  es_client = Elasticsearch(\n"
     ]
    }
   ],
   "source": [
    "es_client = Elasticsearch(\n",
    "    hosts=[{'host': 'localhost', 'port': 9200, 'scheme': 'http'}],\n",
    "    timeout=30,\n",
    "    max_retries=10,\n",
    "    retry_on_timeout=True\n",
    ")\n",
    "\n",
    "# Prueba la conexión\n",
    "if es_client.ping():\n",
    "    print(\"Conectado a Elasticsearch!\")\n",
    "else:\n",
    "    print(\"No se pudo conectar a Elasticsearch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.indices.create(index=index_name, body=index_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:04<00:00, 220.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in notebook_tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do i run docker?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"], #the power gives more importance to that part\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\" #we add a filter for one of the keywords on the documents \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag2(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Docker, you need to follow these steps:\\n\\n1. **Install Docker**: First, ensure that Docker is installed on your system. You can download and install Docker from the official Docker website or use a package manager specific to your operating system.\\n\\n2. **Run Docker Containers**: Once Docker is installed, you can run Docker containers using the `docker run` command. For example:\\n   ```sh\\n   docker run <image_name>\\n   ```\\n\\n3. **Check Docker Installation**: To verify that Docker is installed correctly, you can run:\\n   ```sh\\n   docker --version\\n   ```\\n\\n4. **Pull Docker Image**: If you need a specific Docker image, you can pull it from Docker Hub using:\\n   ```sh\\n   docker pull <image_name>\\n   ```\\n\\n5. **Build Docker Image**: If you have a Dockerfile, you can build a Docker image using:\\n   ```sh\\n   docker build -t <image_name> .\\n   ```\\n\\n6. **Run Docker Container from Image**: After pulling or building an image, you can run a container from it using:\\n   ```sh\\n   docker run <image_name>\\n   ```\\n\\n7. **Check Running Containers**: To see a list of running containers, use:\\n   ```sh\\n   docker ps\\n   ```\\n\\n8. **Stop and Remove Containers**: To stop a running container, use:\\n   ```sh\\n   docker stop <container_id>\\n   ```\\n   To remove a container, use:\\n   ```sh\\n   docker rm <container_id>\\n   ```'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
